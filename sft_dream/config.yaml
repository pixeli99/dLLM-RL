wandb:
  resume: auto
  run_id: t6cc3rb5
experiment:
  project: sft_dream
  num_node: 1
  logging_dir: sft_dream/logs
model:
  pretrained_model: Dream-org/Dream-v0-Instruct-7B
  optimized_name: optimized
dataset:
  optimization_data: s1k_sft_final
training:
  gradient_checkpointing_enable: true
  gradient_accumulation_steps: 2
  batch_size_lm: 2
  mixed_precision: bf16
  enable_tf32: true
  seed: 10086
  num_train_epochs: 3
  max_grad_norm: 1
  method: random_masking
  lower_p: 0.1
  upper_p: 0.9
  block_size: 16
  use_block_mask: false
  mask_times_per_sample: 35
  post_num: 0
  max_gen_length: 8192
  max_prompt_len: 1024
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    scale_lr: false
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    epsilon: 1.0e-08
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 0
    min_lr_scale: 1.0
tokenizer:
  additional_special_tokens:
  - <think 1>
  - </think 1>
  - <think 2>
  - </think 2>
  - <think 3>
  - </think 3>
  - <think 4>
  - </think 4>
  - <think 5>
  - </think 5>
  - <think 6>
  - </think 6>
  - <summary>
  - </summary>
  - <delete>
config: configs/sft_dream.yaml
